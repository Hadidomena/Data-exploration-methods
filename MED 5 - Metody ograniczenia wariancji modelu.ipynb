{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MED 5\n",
    "\n",
    "# Metody ograniczenia wariancji modelu\n",
    "\n",
    "Poszukując modelu regresji czy klasyfikacji staramy się, by powstały estymator posiadał porządane statystycznie cechy: był zgodny, nieobciążony i najbardziej efektywny. W rzeczywistości trudno jest pogodzić dwie ostatnie własności. Przez brak obciążenia (*ang. bias*) rozumie się bowiem jak najlepsze dopasowanie do danych uczących. Przez największą efektywność rozumie się zwykle jak najmniejszy błąd estymacji dla zbioru testowego (każdego innego niż uczący) - najmniejszą wariancję modelu. Dla rzeczywistych zbiorów danych, na ogół, im mniejsze jest obciążenie, tym jest większa wariancja modelu i odwrotnie. Z uwagi jednak na fakt, iż celem dla którego powstaje model estymatora jest jak najskuteczniejsze jego działanie dla nowych danych (innych niż uczące), przy tworzeniu modelu stosuje się metody, które kosztem zwiększenia obciążenia, zmniejszają wariancję modelu. W praktyce jest to osiągane przez poszukiwanie, w procesie uczenia,  modelu na tyle prostego by właściwie odwzorować trend, niekoniecznie idealnie dopasowującego się do danych zbioru uczącego.  \n",
    "\n",
    "Zwiększając liczbę cech modelu oraz tworząc nowe zmienne opisujące zwiększa się zwykle jego wariancja. Widać to wyraźnie w przypadku zastosowania regresji wielomianowej. Zwiększanie stopnia wielomianu opisującego trend naszych danych zwiększane zostają współczynniki stojące przy kolejnych potęgach, co pozwala na dokładne dopasowanie modelu do danych uczących, jednak kosztem zwiększenia jego wariancji. Mimo iż obciążenie modelu z punktu widzenia danych uczących spada do zera, to tracimy jego zdolności uogólnienia (błąd estymacji wartości na podstawie zbioru testowego rośnie), co oznacza przeuczenie/nadmierne dopasowanie (ang. overfitting) naszego modelu.\n",
    "\n",
    "Sposobem na redukcję stopnia przeuczenia/nadmiernego dopasowania modelu, a zatem ograniczenia jego wariancji, w przypadku regresji wielomianowej, są techniki regularyzacji. Generalnie mamy dwie możliwości działania:\n",
    "\n",
    "1. Redukujemy wpływ cech nieznaczących na model tj. pozostawiamy wszystkie cechy ale w modelu realny wpływ na wyjście mają jedynie cechy znaczące - działa jeśli mamy dużo cech, z których każda skorelowana jest z wyjściem (regularyzacja Tichonowa, regresja grzbietowa - ang. ridge regression).\n",
    "       \n",
    "2. Redukujemy liczbę cech na jeden z trzech sposobów:\n",
    "\n",
    "    a) selekcja manualna - używamy ważniejszych cech (w przypadku unormowanych wartości atrybutów opisujących będą to cechy, przy których wartość modułu współczynnika jest najwyższa)    \n",
    "    \n",
    "    b) używamy algorytmu do wyboru cech modelu (metoda zachłanna - analizujemy które cechy najlepiej minimalizują sumę kwadratu błędów)\n",
    "    \n",
    "    c) używamy metody regularyzacji eliminującej zbędne cechy (metoda LASSO). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresja grzbietowa i LASSO (Least Absolute Shrinkage and Selection Operator) to efektywne techniki zwykle stosowane do tworzenia oszczędnych modeli w obecności „dużej” liczby funkcji. Tutaj „duże” może zwykle oznaczać jedną z dwóch rzeczy:\n",
    "\n",
    "1. Wystarczająco duży, aby zwiększyć tendencję modelu do nadmiernego dopasowania (tak niskie, jak 10 zmiennych może powodować nadmierne dopasowanie)\n",
    "    \n",
    "2. Wystarczająco duży, aby powodować wyzwania obliczeniowe. W przypadku nowoczesnych systemów taka sytuacja może wystąpić w przypadku milionów lub miliardów cech.\n",
    "\n",
    "Chociaż metoda grzbietowa i Lasso mogą wydawać się bardzo do siebie  podobne, dzięki specyficznym własnościom ich praktyczne zastosowania różnią się znacznie. Działają one poprzez karanie wielkości współczynników cech oraz minimalizowanie błędu między przewidywanymi a rzeczywistymi obserwacjami. Kluczowa różnica polega na sposobie przypisywania kary do współczynników:\n",
    "\n",
    "* Regresja grzbietowa (ridge regression, regularyzacja Tichonowa):\n",
    "        Dokonuje normalizacji L2, tj. Dodaje karę równą kwadratowi wielkości współczynników\n",
    "        Cel minimalizacji = LS Obj + α * (suma kwadratów współczynników)\n",
    "* Regresja lasso:\n",
    "        Dokonuje normalizacji L1, tj. Dodaje karę równoważną wartości bezwzględnej wielkości współczynników\n",
    "        Cel minimalizacji = LS Obj + α * (suma wartości bezwzględnych współczynników)\n",
    "\n",
    "„LS Obj” odnosi się do „celu najmniejszych kwadratów”, tj. Celu regresji liniowej bez regularyzacji.\n",
    "\n",
    "**Celem tego laboratorium jest zapoznanie się z działaniem metod regularyzacji i dobru cech modelu**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materiały pomocnicze, lekkie (wideo) wprowadzenia do:\n",
    "\n",
    "* [Ridge regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "* [Metody lasso](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "* [i połączenie obu (elastic net)](https://www.youtube.com/watch?v=1dKRdX9bfIo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek - wczytanie bibliotek oraz wygenerowanie zbioru danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj bilbioteki podstawowe\n",
    "import math \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj biblioteki potrzebne do zajęć\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "x = 2 - 3 * np.random.normal(0, 0.5, 50)\n",
    "y = 30 + x - 5 * (x ** 2) - 2 * (x ** 3)+ 0.15 * (x ** 5)  + np.random.normal(-3, 5, 50)\n",
    "plt.scatter(x,y, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1 - regresja grzbietowa (ridge regression)\n",
    "\n",
    "Zadanie polega na znalezieniu wielomianu $\\hat{f}(x)$ postaci:\n",
    "\n",
    "$$\\hat{f}(x)=\\omega_0+\\omega_1 x^1+\\omega_2x^2+ … +\\omega_m x^m,$$\n",
    "\n",
    "który mimo wysokiej złożoności modelu nie powoduje nadmiernego dopasowania do danych uczących poprzez zastosowanie regularyzacji L2.\n",
    "\n",
    "Chcemy by nasza funkcja kosztu $J(\\omega)$ miała postać:\n",
    "\n",
    "$$J_{ridge}(\\omega)=\\sum_{i=0}^{n-1}\\left(\\hat{f}(x_i)−y_i\\right)^2+\\lambda \\sum_{j=1}^m{\\omega^2_j},$$\n",
    "\n",
    "gdzie $i$ oznacza numer obserwacji, a $j$ kolejność współczynnika.\n",
    "\n",
    "Wtedy rozwiązanie naszego równania wyzaczającego współczynniki wygląda w następujący sposób:\n",
    "\n",
    "$$ \\omega=\\left(X^T X + \\lambda \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & \\ddots & 0\\\\ \n",
    "0 & 0 & 0  & 1 \\\\ \n",
    "\\end{bmatrix}\\right) ^{-1} X^Ty $$\n",
    "\n",
    "**Należy pamiętać, że wyrazu wolnego nie poddajemy regularyzacji !**\n",
    "\n",
    "A zatem - do dzieła!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ramach pierwszej części zadania należy napisać funkcję wyznaczającą współczynniki $\\omega$ wielomianowej funkcji regresji zgodnie z powyższym wzorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Napisz funkcję dopasowującą model regresji wielomianowej z regularyzacją L2 \n",
    "# (wykorzystując rozwiązanie równania normalnego)\n",
    "def regresja_L2(X, y, lamb, stopien_wielomianu):\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poprawne (lub nie) działanie powyższej funkcji należy sprawdzić rysując jej wykres na wykresie punktowym zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rysowanie wykresu \n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, w*X, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobnie jak wiele innych metod uczenia maszynowego, także i metoda regresji grzbietowej została zaimplementowana w jednym z pakietów języka Python. W kolejnym kroku należy znaleźć stosowną funkcję oraz porównać wyniki jej działania z własną funkcją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miejsce na kod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem wyznaczenia funkcji regresji na całym zbiorze, tak jak to miało miejsce powyżej, było praktyczne sprawdzenie jej działania. W przypadku, gdy uzyskany model estymatora miałby być zastosowany do estymacji wartości wyjściowej dla nieznanych wartości wejściowych (a zwykle tak to właśnie wygląda), oryginalny zbiór danych powinien zostać podzielony na zbiór uczący i testowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ze zbioru danych wydzielamy zbiór testowy zawierający 20% danych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do oceny modelu w procesie uczenia stosuje się często walidację krzyżową. W omawianym przypadku wyróżnia się dwa parametry metody - stopień wielomianu funkcji regresji oraz parametr $\\lambda$. Zwykle pierwszy z nich jest ustalany a-priori, zaś drugi podlega weryfikacji poprzez wykonanie walidacji krzyżowej estymatorów dla różnych jego wartości. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z walidacji krzyżowej (5-krotny wybór zbioru) na zbiorze treningowym napisz funkcję, która dla wielomianu stopnia 15 narysuje wykres średniego błędu uczenia i średniego błędu walidacji, jak również wykres przedstawiający wartości współczynników modelu w funkcji zmiany parametru regularyzacji $\\lambda=[0, 0.01, 0.02, 0.04, 0.08, \\cdots, 10]$. Można przyjąć, że $\\lambda_{a+1}=24\\lambda_a$. Można skorzystać z funkcji [cross_validation](https://scikit-learn.org/stable/modules/cross_validation.html) lub [Search_Grid_CV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na potrzebne funkcje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytania:**\n",
    "1. Jak zmieniają się wartości współczynników ze zmianą współczynnika regularyzacji $\\lambda$?\n",
    "2. Która wartość współczynnika regularyzacji $\\lambda$ jest najlepsza dla wybranego modelu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyucz opracowany model (stopnia 15) dla wybranego współczynnik $\\lambda$ (uczenie na całym zbiorze treningowym). Wyznacz $\\mbox{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{f}(x_i)−y_i\\right)^2}$ (pierwiastek błędu średniokwadratowego) dla zbioru uczącego i testowego i porównaj z wynikiem otrzymanym dla modelu stopnia 5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na potrzebne funkcje\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2 - Metoda doboru cech modelu\n",
    "\n",
    "Pierwszą metodą doboru cech modelu, która zostanie przetestowana jest metoda zachłanna (algorytm w przód). W każdym kroku tej metody poszukujemy cechy, która najlepiej minimalizuje nam średni błąd CV (walidacji krzyżowej), a następnie dodajemy ją do zbioru cech.   \n",
    "\n",
    "Do tego celu wykorzystamy zbiór przedstawiający ceny mieszkań z King County (do pobrania z ISODu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych\n",
    "import pandas as pd\n",
    "\n",
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "\n",
    "sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzenie nowych zmiennych\n",
    "from math import log, sqrt\n",
    "sales['sqft_living_sqrt'] = sales['sqft_living'].apply(sqrt)\n",
    "sales['sqft_lot_sqrt'] = sales['sqft_lot'].apply(sqrt)\n",
    "sales['bedrooms_square'] = sales['bedrooms']*sales['bedrooms']\n",
    "sales['floors_square'] = sales['floors']*sales['floors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ze zbioru danych wydzielamy zbiór testowy zawierający 20% danych.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na zbiorze treningowym, dla cech \n",
    "['bedrooms', 'bedrooms_square', 'bathrooms', 'sqft_living', 'sqft_living_sqrt', 'sqft_lot', 'sqft_lot_sqrt', 'floors', 'floors_square', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "przy pomocy walidacji krzyżowej wyznaczamy wykresy błąd modelu, który będzie miał tylko wyraz wolny, a następnie biorąc pojedynczą cechę, a następnie wszystkie podwójne, potrójne itd. cechy wyznaczamy błędy modelu. Na podstawie tych błędów wyznaczamy, które zbiory cech (1, 2, 3, ...) elementowe nalepiej minimalizują nam błąd CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na kod\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "features =  ['bedrooms', 'bedrooms_square', 'bathrooms', 'sqft_living', 'sqft_living_sqrt', 'sqft_lot', 'sqft_lot_sqrt', 'floors', 'floors_square', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "for i, value in enumerate(features):\n",
    "    tmp_features = \n",
    "    lr.fit(sales[tmp_features], sales['price'])\n",
    "    error = \n",
    "# Wypisz zestawy cech najlepiej minimalizujące błąd CV dla różnej ilości cech. (Wyraz wolny pozostawiamy.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie wykorzystując powyższe wyniki lub opracowując nową funkcję napisz algorytm zachłanny wyboru cech, który do nowego zbioru cech dorzucać będzie cechę, która najlepiej minimalizuje błąd naszego modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejce na kod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prowównaj jak powyżej otrzymane cechy dla algorytmu zachłannego różnią się od najlepszych zbiorów cech.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3 - Metoda Lasso\n",
    "W metodzie LASSO (Least Absolute Shrinkage and Selection Operator) wykorzystujemy normę L1 do wygaszenia cech, które nia mają znaczącego wpływu na model. W przypadku tej metody funkcja kosztu C($\\omega$) ma następującą postać:\n",
    "\n",
    "$$C_{lasso}(\\omega)=\\sum_{i=1}^{n}\\left(\\hat{f}(x_i)−y_i\\right)^2+\\lambda \\sum_{j=1}^m{|\\omega_j|},$$\n",
    "\n",
    "W przypadku regularyzacji metodą Lasso dokonuje się normalizacji zmiennych opisujących. \n",
    "\n",
    "Poniżej przykładowy kod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import linear_model  # using scikit-learn\n",
    "\n",
    "all_features = ['bedrooms', 'bedrooms_square',\n",
    "            'bathrooms',\n",
    "            'sqft_living', 'sqft_living_sqrt',\n",
    "            'sqft_lot', 'sqft_lot_sqrt',\n",
    "            'floors', 'floors_square',\n",
    "            'waterfront', 'view', 'condition', 'grade',\n",
    "            'sqft_above',\n",
    "            'sqft_basement',\n",
    "            'yr_built', 'yr_renovated']\n",
    "\n",
    "model_all = linear_model.Lasso(alpha=5e2, normalize=True) # set parameters\n",
    "model_all.fit(sales[all_features], sales['price']) # learn weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Na podstawie wyników powyższego kodu określ, które cechy zostały wybrane przez algorytm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak samo jak działaliśmy w przypadku regularyzacji grzbietowej, wydziel 20% zbiór testowy, a następnie przy pomocy CV na zbiorze zbiorze uczącym i powyższego kodu, dokonaj doboru współczynnika alpha. Wykreś RMSE oraz współczynniki poszczególnych cech w funkcji zmiany parametru $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na kod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytania:**\n",
    "1. Jaką wartość parametru $\\lambda$ wybrałbyś/wybrałabyś dla naszego modelu ?\n",
    "2. Które z cech, dla ustalonej wartości parametru $\\lambda$ wchodzą w skład modelu - podaj w kolejności od najbardziej znaczących cech ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na kod\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
